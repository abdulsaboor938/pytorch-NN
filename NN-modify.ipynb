{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the libraries and depenedencies\n",
    "\n",
    "# !pip install -qqq pandas\n",
    "# !pip install -qqq scikit-learn\n",
    "# !pip install -qqq flwr\n",
    "# !pip install -qqq tensorflow\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing roll-pitch-yaw\n",
    "M_PI=3.1416\n",
    "def compute_roll_yaw_pitch(x,y,z):\n",
    "  #Acceleration around X\n",
    "  acc_x_accl=[]\n",
    "\n",
    "  #Acceleration around Y\n",
    "  acc_y_accl=[]\n",
    "\n",
    "  #Acceleration arounf Z\n",
    "  acc_z_accl=[]\n",
    "\n",
    "\n",
    "  for (x_mean,y_mean,z_mean) in zip(x,y,z):\n",
    "    acc_x_accl.append(float(\"{:.2f}\".format(x_mean*3.9)))\n",
    "    acc_y_accl.append(float(\"{:.2f}\".format(y_mean*3.9)))\n",
    "    acc_z_accl.append(float(\"{:.2f}\".format(z_mean*3.9)))\n",
    "\n",
    "\n",
    "  acc_pitch=[]\n",
    "  acc_roll=[]\n",
    "  acc_yaw=[]\n",
    "\n",
    "  for (acc_x,acc_y,acc_z) in zip(acc_x_accl,acc_y_accl,acc_z_accl):\n",
    "    if acc_y==0 and acc_z==0:\n",
    "      value_pitch=-0.1\n",
    "    else:\n",
    "      value_pitch=180 * math.atan (acc_x/math.sqrt(acc_y*acc_y + acc_z*acc_z))/M_PI\n",
    "    if acc_x ==0 and acc_z==0:\n",
    "      value_roll=-0.1\n",
    "      value_yaw=-0.1\n",
    "    else:\n",
    "      value_roll = 180 * math.atan (acc_y/math.sqrt(acc_x*acc_x + acc_z*acc_z))/M_PI\n",
    "      value_yaw = 180 * math.atan (acc_z/math.sqrt(acc_x*acc_x + acc_z*acc_z))/M_PI\n",
    "    value_pitch=float(\"{:.2f}\".format(value_pitch))\n",
    "    value_roll=float(\"{:.2f}\".format(value_roll))\n",
    "    value_yaw=float(\"{:.2f}\".format(value_yaw))\n",
    "    acc_pitch.append(value_pitch)\n",
    "    acc_roll.append(value_roll)\n",
    "    acc_yaw.append(value_yaw)\n",
    "  return acc_pitch,acc_roll,acc_yaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sliding Window to null values\n",
    "def fill_null(data):\n",
    "  for col in data.columns:\n",
    "    null_indexes=data[data[col].isnull()].index.tolist()\n",
    "    #print(\"For \",col)\n",
    "    for ind in null_indexes:\n",
    "      #print(\" Got null value at \",ind)\n",
    "      values=data.loc[ind-6:ind-1,col]\n",
    "      #print(\" Last 5 values \",values)\n",
    "      mean_val=values.mean()\n",
    "      data.loc[ind,col]=mean_val\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data\n",
    "def load_data(path):\n",
    "    df_one=pd.read_csv(path)\n",
    "    #accelerometer\n",
    "    df_acc=df_one.iloc[:,1:27]\n",
    "    df_acc=fill_null(df_acc)\n",
    "    #gyroscope\n",
    "    df_gyro=df_one.iloc[:,27:53]\n",
    "    df_gyro=fill_null(df_gyro)\n",
    "    #magnometer\n",
    "    df_magnet=df_one.iloc[:,53:84]\n",
    "    df_magnet=fill_null(df_magnet)\n",
    "    # watch accelerometer\n",
    "    #df_watch_acc=df_one.iloc[:,84:130]\n",
    "    # location\n",
    "    #df_location=df_one.iloc[:,139:156]\n",
    "\n",
    "    # For accelerometer\n",
    "    #mean values\n",
    "    acc_mean_x=df_acc['raw_acc:3d:mean_x']\n",
    "    acc_mean_y=df_acc['raw_acc:3d:mean_y']\n",
    "    acc_mean_z=df_acc['raw_acc:3d:mean_z']\n",
    "\n",
    "    acc_mean_x=acc_mean_x.replace({0:0.001})\n",
    "\n",
    "    #standard deviations\n",
    "    #acc_std_x=df_acc['raw_acc:3d:std_x']\n",
    "    #acc_std_y=df_acc['raw_acc:3d:std_y']\n",
    "    #acc_std_z=df_acc['raw_acc:3d:std_z']\n",
    "\n",
    "    (pitch,roll,yaw)=compute_roll_yaw_pitch(acc_mean_x,acc_mean_y,acc_mean_z)\n",
    "    df_one['acc_pitch']=pitch\n",
    "    df_one['acc_roll']=roll\n",
    "    df_one['acc_yaw']=yaw\n",
    "\n",
    "    #for gyroscope\n",
    "    gyro_mean_x=df_gyro['proc_gyro:3d:mean_x']\n",
    "    gyro_mean_y=df_gyro['proc_gyro:3d:mean_y']\n",
    "    gyro_mean_z=df_gyro['proc_gyro:3d:mean_z']\n",
    "\n",
    "    (pitch,roll,yaw)=compute_roll_yaw_pitch(gyro_mean_x,gyro_mean_y,gyro_mean_z)\n",
    "\n",
    "    df_one['gyro_pitch']=pitch\n",
    "    df_one['gyro_roll']=roll\n",
    "    df_one['gyro_yaw']=yaw\n",
    "\n",
    "    # For magnetometer\n",
    "    magno_mean_x=df_magnet['raw_magnet:3d:mean_x']\n",
    "    magno_mean_y=df_magnet['raw_magnet:3d:mean_y']\n",
    "    magno_mean_z=df_magnet['raw_magnet:3d:mean_z']\n",
    "\n",
    "    (pitch,roll,yaw)=compute_roll_yaw_pitch(magno_mean_x,magno_mean_y,magno_mean_z)\n",
    "\n",
    "    df_one['magno_pitch']=pitch\n",
    "    df_one['magno_roll']=roll\n",
    "    df_one['magno_yaw']=yaw\n",
    "\n",
    "    y=df_one[['label:FIX_running','label:FIX_walking','label:SITTING','label:SLEEPING','label:OR_standing']]\n",
    "\n",
    "    # to avoid null values\n",
    "    y['label:FIX_running']=y['label:FIX_running'].fillna(0)\n",
    "    y['label:FIX_walking']=y['label:FIX_walking'].fillna(0)\n",
    "    y['label:SITTING']=y['label:SITTING'].fillna(0)\n",
    "    y['label:SLEEPING']=y['label:SLEEPING'].fillna(0)\n",
    "    y['label:OR_standing']=y['label:OR_standing'].fillna(0)\n",
    "\n",
    "    #Check rows where all the recorded activities are zero ~ No activity recorded rows\n",
    "    list_of_indexs=[]\n",
    "    for i in range(len(y)):\n",
    "        run=y.iloc[i,0]\n",
    "        walk=y.iloc[i,1]\n",
    "        sit=y.iloc[i,2]\n",
    "        sleep=y.iloc[i,3]\n",
    "        stand=y.iloc[i,4]\n",
    "        activities=[run,walk,sit,sleep,stand]\n",
    "        count_ones=activities.count(1)\n",
    "        if walk==0 and run==0 and sit==0 and sleep==0 and stand==0:\n",
    "            list_of_indexs.append(i)\n",
    "        #check if more then 1 exists for different activities\n",
    "        elif count_ones>1:\n",
    "            list_of_indexs.append(i)\n",
    "\n",
    "    y=y.drop(list_of_indexs)\n",
    "    X=df_one.iloc[:,-9:]\n",
    "    X=X.drop(list_of_indexs)\n",
    "\n",
    "\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mget_data\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m00EABED2-271D-49D8-B599-1D4A09240601.features_labels.csv.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_data' is not defined"
     ]
    }
   ],
   "source": [
    "X = get_data(\"00EABED2-271D-49D8-B599-1D4A09240601.features_labels.csv.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the PyTorch neural network model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(9, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(16, 8)\n",
    "        self.fc4 = nn.Linear(8, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.leaky_relu(self.fc1(x))\n",
    "        x = nn.functional.leaky_relu(self.fc2(x))\n",
    "        x = nn.functional.leaky_relu(self.fc3(x))\n",
    "        x = nn.functional.softmax(self.fc4(x), dim=1)\n",
    "        return x\n",
    "\n",
    "def init_train(data, dump, num_epochs=100):\n",
    "\n",
    "    # Features and labels\n",
    "    X = data.drop('y', axis=1).values\n",
    "    y = data['y'].values\n",
    "\n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # PyTorch data loaders\n",
    "    train_dataset = torch.utils.data.TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train))\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        \n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    model = SimpleNN()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=0.001, momentum=0.1)\n",
    "\n",
    "    # Train the PyTorch model\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # convert all the model parameters to json\n",
    "    import json\n",
    "    params = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        params[name] = param.data.numpy().tolist()\n",
    "\n",
    "    # save the model parameters to a json file\n",
    "    with open(dump, 'w') as f:\n",
    "        json.dump(params, f)\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(torch.tensor(X_test, dtype=torch.float32))\n",
    "        _, predicted = torch.max(test_outputs, 1)\n",
    "        test_accuracy = accuracy_score(y_test, predicted.numpy())\n",
    "        return('Test accuracy:', test_accuracy)\n",
    "    \n",
    "def re_train(data, checkpoint , dump, num_epochs=100):\n",
    "\n",
    "    # Features and labels\n",
    "    X = data.drop('y', axis=1).values\n",
    "    y = data['y'].values\n",
    "\n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # PyTorch data loaders\n",
    "    train_dataset = torch.utils.data.TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train))\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        \n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    model = SimpleNN()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=0.001, momentum=0.1)\n",
    "\n",
    "    # load the model parameters from a json file\n",
    "    with open(checkpoint, 'r') as f:\n",
    "        params = json.load(f)\n",
    "\n",
    "    # convert the model parameters from json to tensors\n",
    "    for name, param in model.named_parameters():\n",
    "        param.data = torch.tensor(params[name])\n",
    "\n",
    "    # Train the PyTorch model\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # convert all the model parameters to json\n",
    "    params = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        params[name] = param.data.numpy().tolist()\n",
    "\n",
    "    # save the model parameters to a json file\n",
    "    with open(dump, 'w') as f:\n",
    "        json.dump(params, f)\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(torch.tensor(X_test, dtype=torch.float32))\n",
    "        _, predicted = torch.max(test_outputs, 1)\n",
    "        test_accuracy = accuracy_score(y_test, predicted.numpy())\n",
    "        return('Test accuracy:', test_accuracy)\n",
    "    \n",
    "def predict(data, checkpoint):\n",
    "    \n",
    "    # Features and labels\n",
    "    X_test = data.drop('y', axis=1).values\n",
    "    y_test = data['y'].values\n",
    "\n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "        \n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    model = SimpleNN()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=0.001, momentum=0.1)\n",
    "\n",
    "    # load the model parameters from a json file\n",
    "    with open(checkpoint, 'r') as f:\n",
    "        params = json.load(f)\n",
    "\n",
    "    # convert the model parameters from json to tensors\n",
    "    for name, param in model.named_parameters():\n",
    "        param.data = torch.tensor(params[name])\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(torch.tensor(X_test, dtype=torch.float32))\n",
    "        _, predicted = torch.max(test_outputs, 1)\n",
    "        test_accuracy = accuracy_score(y_test, predicted.numpy())\n",
    "        return('Test accuracy:', test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

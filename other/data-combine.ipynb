{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path):\n",
    "    X,y=load_data(path)\n",
    "\n",
    "    # making a single column for all the activities\n",
    "    y['activity']=y.idxmax(axis=1)\n",
    "\n",
    "    # convering the categorical data into numerical data\n",
    "    y['activity']=y['activity'].replace({'label:FIX_walking':0,'label:SITTING':1,'label:SLEEPING':2,'label:OR_standing':3,'label:FIX_running':4})\n",
    "\n",
    "    y = y['activity']\n",
    "\n",
    "    X['y'] = y\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(\"00EABED2-271D-49D8-B599-1D4A09240601.features_labels.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['11B5EC4D-4133-4289-B475-4E737182A406.features_labels.csv.gz',\n",
       " '27E04243-B138-4F40-A164-F40B60165CF3.features_labels.csv.gz',\n",
       " '5EF64122-B513-46AE-BCF1-E62AAC285D2C.features_labels.csv.gz',\n",
       " '7D9BB102-A612-4E2A-8E22-3159752F55D8.features_labels.csv.gz',\n",
       " '9DC38D04-E82E-4F29-AB52-B476535226F2.features_labels.csv.gz',\n",
       " '7CE37510-56D0-4120-A1CF-0E23351428D2.features_labels.csv.gz',\n",
       " '1DBB0F6F-1F81-4A50-9DF4-CD62ACFA4842.features_labels.csv.gz',\n",
       " '24E40C4C-A349-4F9F-93AB-01D00FB994AF.features_labels.csv.gz',\n",
       " '0A986513-7828-4D53-AA1F-E02D6DF9561B.features_labels.csv.gz',\n",
       " '2C32C23E-E30C-498A-8DD2-0EFB9150A02E.features_labels.csv.gz',\n",
       " '00EABED2-271D-49D8-B599-1D4A09240601.features_labels.csv.gz',\n",
       " '4E98F91F-4654-42EF-B908-A3389443F2E7.features_labels.csv.gz',\n",
       " '0E6184E1-90C0-48EE-B25A-F1ECB7B9714E.features_labels.csv.gz',\n",
       " '4FC32141-E888-4BFF-8804-12559A491D8C.features_labels.csv.gz',\n",
       " '0BFC35E2-4817-4865-BFA7-764742302A2D.features_labels.csv.gz']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list all data files with .gz extension\n",
    "import glob\n",
    "filenames = glob.glob(\"*.gz\")\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3\n",
      "0 3\n",
      "0 3\n",
      "0 3\n",
      "0 4\n",
      "0 4\n",
      "0 4\n",
      "0 3\n",
      "0 4\n",
      "0 3\n",
      "0 3\n",
      "0 3\n",
      "0 3\n",
      "0 4\n",
      "0 4\n"
     ]
    }
   ],
   "source": [
    "data = get_data(filenames[0])\n",
    "print(data['y'].min(),data['y'].max())\n",
    "data.to_csv('train.csv', index=False)\n",
    "# print(\"data.csv created\")\n",
    "for i in filenames[1:-1]:\n",
    "    data = get_data(i)\n",
    "    print(data['y'].min(),data['y'].max())\n",
    "    # appending all the data files into a single csv\n",
    "    data.to_csv('train.csv', mode='a', header=False, index=False)\n",
    "    # print(i+\" appended\")\n",
    "data = get_data(filenames[-1])\n",
    "print(data['y'].min(),data['y'].max())\n",
    "data.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now testing the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import json\n",
    "\n",
    "# Define the PyTorch neural network model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(9, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(16, 8)\n",
    "        self.fc4 = nn.Linear(8, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.leaky_relu(self.fc1(x))\n",
    "        x = nn.functional.leaky_relu(self.fc2(x))\n",
    "        x = nn.functional.leaky_relu(self.fc3(x))\n",
    "        x = nn.functional.softmax(self.fc4(x), dim=1)\n",
    "        return x\n",
    "    \n",
    "# Features and labels\n",
    "X = data.drop('y', axis=1).values\n",
    "y = data['y'].values\n",
    "\n",
    "# standardize the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# PyTorch data loaders\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y))\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SimpleNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.001, momentum=0.1)\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# convert all the model parameters to json\n",
    "params = {}\n",
    "for name, param in model.named_parameters():\n",
    "    params[name] = param.data.numpy().tolist()\n",
    "\n",
    "# save the model parameters to a json file\n",
    "with open('trained13.json', 'w') as f:\n",
    "    json.dump(params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Test accuracy:', 0.1802018260451706)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "# loading test data\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "predict(test_data, 'trained13.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Test accuracy:', 0.16546762589928057)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_train(test_data, 'trained13.json', 'trained14.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
